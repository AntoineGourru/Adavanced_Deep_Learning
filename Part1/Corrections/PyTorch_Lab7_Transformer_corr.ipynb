{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744eea0c",
   "metadata": {},
   "source": [
    "# ðŸ§ª PyTorch Lab 7: Language Modeling\n",
    "\n",
    "\n",
    "This is a very large lab session. We will go over manipulating textual data with deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff44fd",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92e2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "import nltk\n",
    "\n",
    "#You might need to download nltlk and run these once : \n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961c254",
   "metadata": {},
   "source": [
    "## 1. Download this book from projet gutenberg\n",
    "We'll start by dowloading Alice in wonderland from the gutenberg project. Just use my code and check what it outputs. It's quite easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c5b772-ed06-46a0-aa6b-cad77180e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.gutenberg.org/ebooks/11.txt.utf-8\"\n",
    "OUT_SENTENCES = Path(\"Alice_in_Wonderland_sentences.txt\")\n",
    "OUT_WORDS = Path(\"Alice_in_Wonderland_words.txt\")\n",
    "\n",
    "\n",
    "def strip_gutenberg_boilerplate(text: str) -> str:\n",
    "    start_marks = [\n",
    "        \"*** START OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "        \"***START OF THE PROJECT GUTENBERG EBOOK\",\n",
    "        \"*** START OF THE PROJECT GUTENBERG EBOOK\",\n",
    "    ]\n",
    "    end_marks = [\n",
    "        \"*** END OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "        \"***END OF THE PROJECT GUTENBERG EBOOK\",\n",
    "        \"*** END OF THE PROJECT GUTENBERG EBOOK\",\n",
    "    ]\n",
    "\n",
    "    start_idx = 0\n",
    "    for mark in start_marks:\n",
    "        i = text.find(mark)\n",
    "        if i != -1:\n",
    "            start_idx = text.find(\"\\n\", i) + 1\n",
    "            break\n",
    "\n",
    "    end_idx = len(text)\n",
    "    for mark in end_marks:\n",
    "        i = text.find(mark)\n",
    "        if i != -1:\n",
    "            end_idx = text.rfind(\"\\n\", 0, i)\n",
    "            break\n",
    "\n",
    "    return text[start_idx:end_idx].strip()\n",
    "\n",
    "with urlopen(URL) as r:\n",
    "    raw = r.read().decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "cleaned = strip_gutenberg_boilerplate(raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f80caae-8e48-470b-8b5f-37e53ef075dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1028, 56, 1761, 32, 1040, 2410, 267, 1189, 291, 2113, 1309, 825, 613, 405, 318, 1020, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<start> illustration alice s adventures in wonderland by lewis carroll the millennium fulcrum edition contents chapter i <stop> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "START_ID = 2\n",
    "STOP_ID = 3\n",
    "\n",
    "def tokenize_words(s: str):\n",
    "    # one place to define your tokenization rules\n",
    "    return [w for w in nltk.word_tokenize(s.lower()) if w.isalpha()]\n",
    "    \n",
    "def text_to_sentences(text: str):\n",
    "    text = \" \".join(text.split())  # normalize whitespace\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "\n",
    "def build_vocab_from_sentences(sentences):\n",
    "    vocab = sorted({w for s in sentences for w in tokenize_words(s)})\n",
    "    # leave 0 for PAD and 1 for UNK\n",
    "    word2idx = {w: i + 4 for i, w in enumerate(vocab)}\n",
    "    word2idx[\"<unk>\"] = UNK_ID\n",
    "    word2idx[\"<start>\"] = START_ID\n",
    "    word2idx[\"<stop>\"] = STOP_ID\n",
    "    word2idx[\"<pad>\"] = PAD_ID\n",
    "    return word2idx, {v: k for k, v in word2idx.items()}\n",
    "\n",
    "def sentences_to_matrix(sentences, word2idx, pad_id=PAD_ID, unk_id=UNK_ID):\n",
    "    tokenized = [tokenize_words(s) for s in sentences]\n",
    "    max_len = max((len(seq) for seq in tokenized), default=0) + 2\n",
    "    matrix = [\n",
    "        [2] + [word2idx.get(w, unk_id) for w in seq] + [3] + [pad_id] * (max_len - len(seq) - 2)\n",
    "        for seq in tokenized\n",
    "    ]\n",
    "    return matrix\n",
    "    \n",
    "\n",
    "def encode(tokens, word2idx):\n",
    "    return [word2idx.get(w, UNK_ID) for w in tokens]\n",
    "\n",
    "def decode(ids, idx2word):\n",
    "    return \" \".join(idx2word.get(i, \"<unk>\") for i in ids)\n",
    "\n",
    "sentences = text_to_sentences(cleaned)\n",
    "word2idx,idx2word = build_vocab_from_sentences(sentences)\n",
    "matrix = sentences_to_matrix(sentences, word2idx)\n",
    "print(matrix[0])\n",
    "print(decode(matrix[0], idx2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32652688-c1d4-4a68-b1a2-d4371b05fc67",
   "metadata": {},
   "source": [
    "## 2. Creation of a simple model for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a422312-a38a-4178-a2e6-a449e32095e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple RNN LM (autoregressive) with PyTorch ---\n",
    "import math\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build dataset: inputs are seq[:-1], targets are seq[1:]\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.seqs = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "        # all rows already padded to same length by sentences_to_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s = self.seqs[i]\n",
    "        x = s[:-1]  # [T-1]\n",
    "        y = s[1:]   # next-token targets [T-1]\n",
    "        return x, y\n",
    "\n",
    "train_ds = LMDataset(matrix)\n",
    "\n",
    "def collate(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    return torch.stack(xs, 0), torch.stack(ys, 0)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc682cb1-a33c-4ebb-b562-7320d6be0213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10 - loss 6.3204 - approx ppl 42277614671946533830656.00\n",
      "epoch 2/10 - loss 5.5697 - approx ppl 790742502726075940864.00\n",
      "epoch 3/10 - loss 5.1748 - approx ppl 25883936047487299584.00\n",
      "epoch 4/10 - loss 4.8449 - approx ppl 1296077791025340160.00\n",
      "epoch 5/10 - loss 4.5472 - approx ppl 101794349844396592.00\n",
      "epoch 6/10 - loss 4.2897 - approx ppl 9263774019950408.00\n",
      "epoch 7/10 - loss 4.0405 - approx ppl 929125014946729.75\n",
      "epoch 8/10 - loss 3.8044 - approx ppl 111862099005958.56\n",
      "epoch 9/10 - loss 3.5809 - approx ppl 16579394498375.17\n",
      "epoch 10/10 - loss 3.3714 - approx ppl 2065627008471.10\n",
      "\n",
      "=== Samples ===\n",
      "once upon a time to be very glad to look in the sea and the baby began to make out of the lobster quadrille the king went on with a pair of mushroom and\n",
      "the king\n"
     ]
    }
   ],
   "source": [
    "class SimpleRNNLM(nn.Module):\n",
    "    def __init__(self, vocab, emb, hid, layers=1, dropout=0.0, padding_idx=PAD_ID):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab, emb, padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=emb,\n",
    "            hidden_size=hid,\n",
    "            num_layers=layers,\n",
    "            batch_first=True,\n",
    "            nonlinearity=\"tanh\",\n",
    "            dropout=dropout if layers > 1 else 0.0,\n",
    "        )\n",
    "        self.proj = nn.Linear(hid, vocab)\n",
    "\n",
    "    def forward(self, x, h0: Optional[torch.Tensor] = None):\n",
    "        # x: [B, T]\n",
    "        emb = self.embed(x)                 # [B, T, E]\n",
    "        out, h = self.rnn(emb, h0)          # out: [B, T, H]\n",
    "        logits = self.proj(out)             # [B, T, V]\n",
    "        return logits, h\n",
    "\n",
    "vocab_size = max(idx2word.keys()) + 1  # idx2word is {id: token}\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "dropout = 0.1\n",
    "lr = 2e-3\n",
    "epochs = 10\n",
    "\n",
    "model = SimpleRNNLM(vocab_size, embedding_dim, hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)  # don't learn on PAD targets\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "def evaluate_perplexity(loader):\n",
    "    model.eval()\n",
    "    total_nll, total_tokens = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device); y = y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            # flatten for CE: [(B*T), V] vs [(B*T)]\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "            # sum nll over non-PAD positions\n",
    "            mask = (y.reshape(-1) != PAD_ID).float()\n",
    "            total_nll += (loss.item() * mask.numel())  # CE returns mean over all positions, adjust below\n",
    "            total_tokens += int(mask.sum().item())\n",
    "    if total_tokens == 0:\n",
    "        return float(\"inf\")\n",
    "    # Recompute using running loss over all items; simpler: do a fresh pass summing token-level nlls\n",
    "    # For simplicity, we approximate perplexity from batch means:\n",
    "    avg_nll = total_nll / max(total_tokens, 1)\n",
    "    return math.exp(avg_nll)\n",
    "\n",
    "# --- Train ---\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    steps = 0\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        logits, _ = model(x)  # [B, T, V]\n",
    "        loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "        running += loss.item()\n",
    "        steps += 1\n",
    "    ppl = evaluate_perplexity(train_loader)\n",
    "    print(f\"epoch {epoch}/{epochs} - loss {running/steps:.4f} - approx ppl {ppl:.2f}\")\n",
    "\n",
    "# --- Autoregressive generation ---\n",
    "@torch.no_grad()\n",
    "def sample(\n",
    "    prompt: str = \"\",\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = 50,\n",
    "    stop_id: int = STOP_ID,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate tokens autoregressively.\n",
    "    - If prompt is empty, we start with <start>.\n",
    "    - Uses single-step RNN updates to carry hidden state.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Build initial context\n",
    "    if prompt.strip():\n",
    "        init_tokens = [START_ID] + encode(tokenize_words(prompt), word2idx)\n",
    "    else:\n",
    "        init_tokens = [START_ID]\n",
    "\n",
    "    x = torch.tensor([init_tokens], dtype=torch.long, device=device)  # [1, T]\n",
    "    # Prime the hidden state by running the context through the RNN\n",
    "    logits, h = model(x)\n",
    "    next_id = int(torch.argmax(logits[0, -1]).item())\n",
    "\n",
    "    generated: List[int] = init_tokens.copy()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Feed only last token each step (true autoregressive)\n",
    "        last = torch.tensor([[generated[-1]]], dtype=torch.long, device=device)  # [1,1]\n",
    "        out, h = model.embed(last), h  # reuse hidden state\n",
    "        out, h = model.rnn(out, h)     # [1,1,H], h updated\n",
    "        logits = model.proj(out[:, -1, :])  # [1,V]\n",
    "        logits = logits / max(temperature, 1e-6)\n",
    "\n",
    "        # Optional top-k filtering\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_vals, top_idx = torch.topk(logits, k=min(top_k, logits.size(-1)), dim=-1)\n",
    "            probs = torch.softmax(top_vals, dim=-1)\n",
    "            next_token = top_idx.gather(-1, torch.multinomial(probs, num_samples=1))\n",
    "        else:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        next_id = int(next_token.item())\n",
    "        generated.append(next_id)\n",
    "        if next_id == stop_id:\n",
    "            break\n",
    "\n",
    "    # Convert to text, skipping specials\n",
    "    specials = {PAD_ID, UNK_ID, START_ID, STOP_ID}\n",
    "    words = [idx2word.get(i, \"<unk>\") for i in generated if i not in specials]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# --- Quick demo ---\n",
    "print(\"\\n=== Samples ===\")\n",
    "print(sample(\"Once upon a time\", max_new_tokens=30, temperature=0.8))\n",
    "print(sample(\"\", max_new_tokens=30, temperature=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0cd663f-ed24-4469-9885-c4c0bbe648f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional(max_L,emb):\n",
    "        posit = torch.zeros(max_L,emb)\n",
    "        for i in range(emb):\n",
    "            if (i % 2) == 0:\n",
    "                posit[0,i] = 0\n",
    "            if (i % 2) == 1:\n",
    "                posit[0,i] = 1\n",
    "        for k in range(1,max_L):\n",
    "            posit[k,0] = 0\n",
    "            for i in range(emb):\n",
    "                if (i % 2) == 0:\n",
    "                    posit[k,i] = math.sin(k/(math.pow(k,(i/emb))))\n",
    "                if (i % 2) == 1:\n",
    "                    posit[k,i] = math.cos(k/(math.pow(k,((i-1)/emb))))\n",
    "        return posit\n",
    "    \n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, max_L, vocab, emb, hid, layers=1, dropout=0.0, padding_idx=PAD_ID):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab, emb, padding_idx=padding_idx)\n",
    "        self.util_emb = math.pow(emb,1/2)\n",
    "        self.positional = create_positional(max_L,emb)\n",
    "        self.key = torch.nn.Linear(emb,emb)\n",
    "        self.query = torch.nn.Linear(emb,emb)\n",
    "        self.value = torch.nn.Linear(emb,emb)\n",
    "        self.proj = torch.nn.Linear(emb,emb)\n",
    "\n",
    "        self.out = nn.Linear(emb, vocab, bias=False)\n",
    "        self.out.weight = self.embed.weight\n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self, x, h0: Optional[torch.Tensor] = None):\n",
    "        # x: [B, T]\n",
    "        x = self.embed(x)                 # [B, T, E]\n",
    "        \n",
    "        pos = self.positional[:x.shape[1],:x.shape[2]]\n",
    "        x += pos\n",
    "        \n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        query = self.query(x)\n",
    "\n",
    "        attention = (query @ key.transpose(-1, -2)) / self.util_emb\n",
    "        mask = torch.tril(torch.ones(attention.shape[1], attention.shape[2]))\n",
    "\n",
    "        attention = attention.masked_fill(mask == 0, float('-inf'))\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        x = x + self.proj(attention @ value)\n",
    "\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "max_L = 512\n",
    "model = SimpleTransformer(max_L, vocab_size, embedding_dim, hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "x,y = next(iter(train_loader))\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e26627-50ec-4872-8f8d-f3d690f5fa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/30 - loss 84.4682 - approx ppl 0.00\n",
      "epoch 2/30 - loss 38.2515 - approx ppl 0.00\n",
      "epoch 3/30 - loss 14.6490 - approx ppl 0.00\n",
      "epoch 4/30 - loss 9.3704 - approx ppl 0.00\n",
      "epoch 5/30 - loss 7.7072 - approx ppl 0.00\n",
      "epoch 6/30 - loss 6.9600 - approx ppl 0.00\n",
      "epoch 7/30 - loss 6.5870 - approx ppl 0.00\n",
      "epoch 8/30 - loss 6.3233 - approx ppl 0.00\n",
      "epoch 9/30 - loss 6.1111 - approx ppl 0.00\n",
      "epoch 10/30 - loss 5.9842 - approx ppl 0.00\n",
      "epoch 11/30 - loss 5.8237 - approx ppl 0.00\n",
      "epoch 12/30 - loss 5.7195 - approx ppl 0.00\n",
      "epoch 13/30 - loss 5.6078 - approx ppl 0.00\n",
      "epoch 14/30 - loss 5.5048 - approx ppl 0.00\n",
      "epoch 15/30 - loss 5.3920 - approx ppl 0.00\n",
      "epoch 16/30 - loss 5.3000 - approx ppl 0.00\n",
      "epoch 17/30 - loss 5.2054 - approx ppl 0.00\n",
      "epoch 18/30 - loss 5.0696 - approx ppl 0.00\n",
      "epoch 19/30 - loss 4.9782 - approx ppl 0.00\n",
      "epoch 20/30 - loss 4.8858 - approx ppl 0.00\n",
      "epoch 21/30 - loss 4.7973 - approx ppl 0.00\n",
      "epoch 22/30 - loss 4.7053 - approx ppl 0.00\n",
      "epoch 23/30 - loss 4.6363 - approx ppl 0.00\n",
      "epoch 24/30 - loss 4.5360 - approx ppl 0.00\n",
      "epoch 25/30 - loss 4.4744 - approx ppl 0.00\n",
      "epoch 26/30 - loss 4.4205 - approx ppl 0.00\n",
      "epoch 27/30 - loss 4.3413 - approx ppl 0.00\n",
      "epoch 28/30 - loss 4.2875 - approx ppl 0.00\n",
      "epoch 29/30 - loss 4.2154 - approx ppl 0.00\n",
      "epoch 30/30 - loss 4.1591 - approx ppl 0.00\n"
     ]
    }
   ],
   "source": [
    "vocab_size = max(idx2word.keys()) + 1  # idx2word is {id: token}\n",
    "embedding_dim = 128\n",
    "lr = 5e-3\n",
    "epochs = 30\n",
    "max_L = 512\n",
    "\n",
    "model = SimpleTransformer(max_L, vocab_size, embedding_dim, hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)  # don't learn on PAD targets\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "def evaluate_perplexity(loader):\n",
    "    model.eval()\n",
    "    total_nll, total_tokens = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device); y = y.to(device)\n",
    "            logits = model(x)\n",
    "            # flatten for CE: [(B*T), V] vs [(B*T)]\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "            # sum nll over non-PAD positions\n",
    "            mask = (y.reshape(-1) != PAD_ID).float()\n",
    "            total_nll += (loss.item() * mask.numel())  # CE returns mean over all positions, adjust below\n",
    "            total_tokens += int(mask.sum().item())\n",
    "    if total_tokens == 0:\n",
    "        return float(\"inf\")\n",
    "    # Recompute using running loss over all items; simpler: do a fresh pass summing token-level nlls\n",
    "    # For simplicity, we approximate perplexity from batch means:\n",
    "    avg_nll = total_nll / max(total_tokens, 1)\n",
    "    return math.exp(avg_nll)\n",
    "\n",
    "# --- Train ---\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    steps = 0\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        logits = model(x)  # [B, T, V]\n",
    "        loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "        running += loss.item()\n",
    "        steps += 1\n",
    "    ppl = 0#evaluate_perplexity(train_loader)\n",
    "    print(f\"epoch {epoch}/{epochs} - loss {running/steps:.4f} - approx ppl {ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfee5283-8225-486f-b589-5878ed21e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Samples ===\n",
      "[[2, 1433, 2274, 4, 2157]]\n",
      "once upon a time the queen pleasant temper of the mock turtle pleasant temper pleasant temper of the hatter and the queen pleasant temper of the door of bathing machines\n",
      "[[2]]\n",
      "and she went on the queen pleasant temper of the mock turtle pleasant temper pleasant temper of the hatter and the queen pleasant temper of the\n"
     ]
    }
   ],
   "source": [
    "# --- Autoregressive generation ---\n",
    "@torch.no_grad()\n",
    "def sample(\n",
    "    prompt: str = \"\",\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = 50,\n",
    "    stop_id: int = STOP_ID,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "    # Build initial context\n",
    "    if prompt.strip():\n",
    "        init_tokens = [START_ID] + encode(tokenize_words(prompt), word2idx)\n",
    "    else:\n",
    "        init_tokens = [START_ID]\n",
    "    seq = [init_tokens]\n",
    "    \n",
    "    print(seq)\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = torch.tensor(seq, dtype=torch.long, device=device)  # [1, T]\n",
    "        # Prime the hidden state by running the context through the RNN\n",
    "        logits = model(x)\n",
    "        next_id = int(torch.argmax(logits[0, -1]).item())\n",
    "        seq = [seq[0] + [next_id]]  \n",
    "        \n",
    "    specials = {PAD_ID, UNK_ID, START_ID, STOP_ID}\n",
    "    words = [idx2word.get(i, \"<unk>\") for i in seq[0] if i not in specials]\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "# --- Quick demo ---\n",
    "print(\"\\n=== Samples ===\")\n",
    "print(sample(\"Once upon a time\", max_new_tokens=30, temperature=0.8))\n",
    "print(sample(\"\", max_new_tokens=30, temperature=1.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
