{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47613d0",
   "metadata": {},
   "source": [
    "# ðŸ§ª PyTorch Lab 3: MLP \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504d1d8",
   "metadata": {},
   "source": [
    "## 0) Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4a67d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d13e24",
   "metadata": {},
   "source": [
    "## 1) Data: load Fashion-MNIST\n",
    "**Exercise 1.1** â€” Load the training and test sets with `ToTensor()` transforms. Keep the default split.\n",
    "\n",
    "Hints:\n",
    "- Use `datasets.FashionMNIST` with `train=True/False`.\n",
    "- Use a local folder like `data/` for `root`.\n",
    "- Set `download=True` on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af84b45",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 12000, 10000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Exercise 1.1 â€” create `training_data` and `test_data`\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, download=True, transform=ToTensor()\n",
    ")\n",
    "\n",
    "training_data, validation_data = train_test_split(training_data, test_size=0.2)\n",
    "\n",
    "len(training_data), len(validation_data),len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b446b",
   "metadata": {},
   "source": [
    "**Exercise 1.2** â€” Visualize one sample image to verify shapes and ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ac20b8",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(27.5), np.float64(27.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Exercise 1.2 â€” visualize a sample\n",
    "x0, y0 = training_data[1]\n",
    "plt.imshow(x0[0].numpy())\n",
    "plt.title(f\"Label: {y0}\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc3198",
   "metadata": {},
   "source": [
    "## 2) Dataloaders\n",
    "**Exercise 2.1** â€” Create dataloaders for train and test with `batch_size=64`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb00a71",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 2.1 â€” make DataLoaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Peek at a single batch\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023e6cd",
   "metadata": {},
   "source": [
    "## 3) Device\n",
    "**Exercise 3.1** â€” Pick `cuda` if available, else `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341ad157",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 3.1 â€” choose device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79151be",
   "metadata": {},
   "source": [
    "## 4) Model: a tiny autoencoder\n",
    "Weâ€™ll encode 28Ã—28 images â†’ 2D latent, then decode back to 28Ã—28.\n",
    "\n",
    "**Exercise 4.1** â€” Implement the following architecture:\n",
    "- Encoder: Linear(784â†’256) â†’ Tanh â†’ Linear(256â†’10)\n",
    "\n",
    "Return the output reshaped to the original image shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16dbc54a",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (first): Linear(in_features=784, out_features=1000, bias=True)\n",
      "  (activation): Tanh()\n",
      "  (second): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 4.1 â€” implement the autoencoder\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.first = nn.Linear(28*28, 1000)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.second = nn.Linear(1000, 10)\n",
    "    def forward(self, x):\n",
    "        initial_shape = x.shape\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.first(x))\n",
    "        x = self.second(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda26c7e-beef-445b-969e-698a2fcc9eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNFashionMNIST(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (activation): ReLU()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CNNFashionMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 1, 28, 28)\n",
    "        x = self.activation(self.conv1(x))  \n",
    "        x = self.pool(x)                   \n",
    "\n",
    "        x = self.activation(self.conv2(x))   \n",
    "        x = self.pool(x)                     \n",
    "\n",
    "        x = self.flatten(x)                  \n",
    "        x = self.activation(self.fc1(x))    \n",
    "        x = self.fc2(x)                      \n",
    "        return x\n",
    "\n",
    "# assuming you already have `device` defined\n",
    "model = CNNFashionMNIST().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d9355",
   "metadata": {},
   "source": [
    "## 5) Loss and Optimizer\n",
    "**Exercise 5.1** â€” Use MSE loss to measure reconstruction error. Choose Adam with learning rate 1e-3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649bd365",
   "metadata": {
    "exercise": true
   },
   "outputs": [],
   "source": [
    "# TODO: Exercise 5.1 â€” define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d5e3e",
   "metadata": {},
   "source": [
    "## 6) Training & Evaluation loops\n",
    "**Exercise 6.1** â€” Implement a standard training loop.\n",
    "\n",
    "**Exercise 6.2** â€” Implement a simple test loop computing loss and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d577860d",
   "metadata": {
    "exercise": true
   },
   "outputs": [],
   "source": [
    "# TODO: Exercise 6.1 â€” training loop (with accuracy)\n",
    "def train(dataloader_val, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Batch accuracy (argmax for multi-class)\n",
    "        with torch.no_grad():\n",
    "            preds = y_pred.argmax(dim=1)\n",
    "            acc = (preds == y).float().mean().item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss_item = loss.item()\n",
    "            current = batch * len(X)\n",
    "            print(f\"loss: {loss_item:>7f} | acc: {acc*100:5.2f}%  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    size = len(dataloader_val.dataset)\n",
    "    num_batches = len(dataloader_val)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader_val:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            preds = pred.argmax(dim=1)\n",
    "            correct += (preds == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "\n",
    "    print(f\"Val Avg loss: {test_loss:>8f} | val accuracy: {accuracy*100:5.2f}%\")\n",
    "\n",
    "# TODO: Exercise 6.2 â€” test loop (with accuracy)\n",
    "def test( dataloader, model, loss_fn):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            preds = pred.argmax(dim=1)\n",
    "            correct += (preds == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Avg loss: {test_loss:>8f} | Test accuracy: {accuracy*100:5.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27d961",
   "metadata": {},
   "source": [
    "**Exercise 6.3** â€” Train for a few epochs (e.g., 5) and observe the printed losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42da1cac",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303831 | acc:  7.81%  [    0/48000]\n",
      "loss: 0.668267 | acc: 76.56%  [ 6400/48000]\n",
      "loss: 0.407611 | acc: 85.94%  [12800/48000]\n",
      "loss: 0.329951 | acc: 90.62%  [19200/48000]\n",
      "loss: 0.440076 | acc: 82.81%  [25600/48000]\n",
      "loss: 0.209810 | acc: 92.19%  [32000/48000]\n",
      "loss: 0.434503 | acc: 82.81%  [38400/48000]\n",
      "loss: 0.513888 | acc: 79.69%  [44800/48000]\n",
      "Val Avg loss: 0.367589 | val accuracy: 86.91%\n",
      "Test Avg loss: 0.390530 | Test accuracy: 86.33%\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.217809 | acc: 89.06%  [    0/48000]\n",
      "loss: 0.195159 | acc: 95.31%  [ 6400/48000]\n",
      "loss: 0.367075 | acc: 85.94%  [12800/48000]\n",
      "loss: 0.373307 | acc: 90.62%  [19200/48000]\n",
      "loss: 0.294066 | acc: 92.19%  [25600/48000]\n",
      "loss: 0.231446 | acc: 89.06%  [32000/48000]\n",
      "loss: 0.288224 | acc: 90.62%  [38400/48000]\n",
      "loss: 0.291981 | acc: 93.75%  [44800/48000]\n",
      "Val Avg loss: 0.295030 | val accuracy: 89.12%\n",
      "Test Avg loss: 0.322277 | Test accuracy: 88.21%\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.458359 | acc: 81.25%  [    0/48000]\n",
      "loss: 0.198926 | acc: 87.50%  [ 6400/48000]\n",
      "loss: 0.191191 | acc: 90.62%  [12800/48000]\n",
      "loss: 0.367961 | acc: 84.38%  [19200/48000]\n",
      "loss: 0.211518 | acc: 95.31%  [25600/48000]\n",
      "loss: 0.208973 | acc: 90.62%  [32000/48000]\n",
      "loss: 0.159508 | acc: 96.88%  [38400/48000]\n",
      "loss: 0.304568 | acc: 87.50%  [44800/48000]\n",
      "Val Avg loss: 0.254107 | val accuracy: 90.79%\n",
      "Test Avg loss: 0.280919 | Test accuracy: 89.73%\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.149149 | acc: 90.62%  [    0/48000]\n",
      "loss: 0.204877 | acc: 93.75%  [ 6400/48000]\n",
      "loss: 0.467964 | acc: 79.69%  [12800/48000]\n",
      "loss: 0.178073 | acc: 90.62%  [19200/48000]\n",
      "loss: 0.205435 | acc: 90.62%  [25600/48000]\n",
      "loss: 0.216457 | acc: 90.62%  [32000/48000]\n",
      "loss: 0.368299 | acc: 89.06%  [38400/48000]\n",
      "loss: 0.279241 | acc: 85.94%  [44800/48000]\n",
      "Val Avg loss: 0.239344 | val accuracy: 91.52%\n",
      "Test Avg loss: 0.265600 | Test accuracy: 90.36%\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.095153 | acc: 95.31%  [    0/48000]\n",
      "loss: 0.142701 | acc: 93.75%  [ 6400/48000]\n",
      "loss: 0.226575 | acc: 87.50%  [12800/48000]\n",
      "loss: 0.311982 | acc: 92.19%  [19200/48000]\n",
      "loss: 0.358850 | acc: 85.94%  [25600/48000]\n",
      "loss: 0.342322 | acc: 87.50%  [32000/48000]\n",
      "loss: 0.172261 | acc: 92.19%  [38400/48000]\n",
      "loss: 0.177609 | acc: 92.19%  [44800/48000]\n",
      "Val Avg loss: 0.232063 | val accuracy: 91.77%\n",
      "Test Avg loss: 0.252447 | Test accuracy: 90.62%\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 6.3 â€” run training\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(val_dataloader, train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Generated by ChatGPT"
   }
  ],
  "created": "2025-09-12T12:05:55.301308Z",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
