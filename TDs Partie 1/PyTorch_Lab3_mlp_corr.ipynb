{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47613d0",
   "metadata": {},
   "source": [
    "# ðŸ§ª PyTorch Lab 3: MLP \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504d1d8",
   "metadata": {},
   "source": [
    "## 0) Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4a67d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d13e24",
   "metadata": {},
   "source": [
    "## 1) Data: load Fashion-MNIST\n",
    "**Exercise 1.1** â€” Load the training and test sets with `ToTensor()` transforms. Keep the default split.\n",
    "\n",
    "Hints:\n",
    "- Use `datasets.FashionMNIST` with `train=True/False`.\n",
    "- Use a local folder like `data/` for `root`.\n",
    "- Set `download=True` on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af84b45",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 12000, 10000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Exercise 1.1 â€” create `training_data` and `test_data`\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, download=True, transform=ToTensor()\n",
    ")\n",
    "\n",
    "training_data, validation_data = train_test_split(training_data, test_size=0.2)\n",
    "\n",
    "len(training_data), len(validation_data),len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b446b",
   "metadata": {},
   "source": [
    "**Exercise 1.2** â€” Visualize one sample image to verify shapes and ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ac20b8",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(27.5), np.float64(27.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Exercise 1.2 â€” visualize a sample\n",
    "x0, y0 = training_data[1]\n",
    "plt.imshow(x0[0].numpy())\n",
    "plt.title(f\"Label: {y0}\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc3198",
   "metadata": {},
   "source": [
    "## 2) Dataloaders\n",
    "**Exercise 2.1** â€” Create dataloaders for train and test with `batch_size=64`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb00a71",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 2.1 â€” make DataLoaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Peek at a single batch\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023e6cd",
   "metadata": {},
   "source": [
    "## 3) Device\n",
    "**Exercise 3.1** â€” Pick `cuda` if available, else `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341ad157",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 3.1 â€” choose device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79151be",
   "metadata": {},
   "source": [
    "## 4) Model: a tiny autoencoder\n",
    "Weâ€™ll encode 28Ã—28 images â†’ 2D latent, then decode back to 28Ã—28.\n",
    "\n",
    "**Exercise 4.1** â€” Implement the following architecture:\n",
    "- Encoder: Linear(784â†’256) â†’ Tanh â†’ Linear(256â†’10)\n",
    "\n",
    "Return the output reshaped to the original image shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16dbc54a",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (first): Linear(in_features=784, out_features=1000, bias=True)\n",
      "  (activation): Tanh()\n",
      "  (second): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 4.1 â€” implement the autoencoder\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.first = nn.Linear(28*28, 1000)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.second = nn.Linear(1000, 10)\n",
    "    def forward(self, x):\n",
    "        initial_shape = x.shape\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.first(x))\n",
    "        x = self.second(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d9355",
   "metadata": {},
   "source": [
    "## 5) Loss and Optimizer\n",
    "**Exercise 5.1** â€” Use MSE loss to measure reconstruction error. Choose Adam with learning rate 1e-3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "649bd365",
   "metadata": {
    "exercise": true
   },
   "outputs": [],
   "source": [
    "# TODO: Exercise 5.1 â€” define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d5e3e",
   "metadata": {},
   "source": [
    "## 6) Training & Evaluation loops\n",
    "**Exercise 6.1** â€” Implement a standard training loop.\n",
    "\n",
    "**Exercise 6.2** â€” Implement a simple test loop computing loss and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d577860d",
   "metadata": {
    "exercise": true
   },
   "outputs": [],
   "source": [
    "# TODO: Exercise 6.1 â€” training loop (with accuracy)\n",
    "def train(dataloader_val, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Batch accuracy (argmax for multi-class)\n",
    "        with torch.no_grad():\n",
    "            preds = y_pred.argmax(dim=1)\n",
    "            acc = (preds == y).float().mean().item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss_item = loss.item()\n",
    "            current = batch * len(X)\n",
    "            print(f\"loss: {loss_item:>7f} | acc: {acc*100:5.2f}%  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    size = len(dataloader_val.dataset)\n",
    "    num_batches = len(dataloader_val)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader_val:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            preds = pred.argmax(dim=1)\n",
    "            correct += (preds == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "\n",
    "    print(f\"Val Avg loss: {test_loss:>8f} | val accuracy: {accuracy*100:5.2f}%\")\n",
    "\n",
    "# TODO: Exercise 6.2 â€” test loop (with accuracy)\n",
    "def test( dataloader, model, loss_fn):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            preds = pred.argmax(dim=1)\n",
    "            correct += (preds == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Avg loss: {test_loss:>8f} | Test accuracy: {accuracy*100:5.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27d961",
   "metadata": {},
   "source": [
    "**Exercise 6.3** â€” Train for a few epochs (e.g., 5) and observe the printed losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42da1cac",
   "metadata": {
    "exercise": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.314828 | acc: 10.94%  [    0/48000]\n",
      "loss: 0.603730 | acc: 79.69%  [ 6400/48000]\n",
      "loss: 0.433276 | acc: 84.38%  [12800/48000]\n",
      "loss: 0.782726 | acc: 75.00%  [19200/48000]\n",
      "loss: 0.413096 | acc: 85.94%  [25600/48000]\n",
      "loss: 0.389725 | acc: 85.94%  [32000/48000]\n",
      "loss: 0.396604 | acc: 85.94%  [38400/48000]\n",
      "loss: 0.329464 | acc: 90.62%  [44800/48000]\n",
      "Val Avg loss: 0.449716 | val accuracy: 83.72%\n",
      "Test Avg loss: 0.473813 | Test accuracy: 82.68%\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.667045 | acc: 79.69%  [    0/48000]\n",
      "loss: 0.498178 | acc: 78.12%  [ 6400/48000]\n",
      "loss: 0.467854 | acc: 90.62%  [12800/48000]\n",
      "loss: 0.396225 | acc: 81.25%  [19200/48000]\n",
      "loss: 0.374385 | acc: 81.25%  [25600/48000]\n",
      "loss: 0.510481 | acc: 76.56%  [32000/48000]\n",
      "loss: 0.299202 | acc: 89.06%  [38400/48000]\n",
      "loss: 0.397375 | acc: 89.06%  [44800/48000]\n",
      "Val Avg loss: 0.393799 | val accuracy: 85.93%\n",
      "Test Avg loss: 0.411958 | Test accuracy: 85.47%\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.408511 | acc: 85.94%  [    0/48000]\n",
      "loss: 0.387530 | acc: 84.38%  [ 6400/48000]\n",
      "loss: 0.345581 | acc: 87.50%  [12800/48000]\n",
      "loss: 0.571614 | acc: 70.31%  [19200/48000]\n",
      "loss: 0.551185 | acc: 84.38%  [25600/48000]\n",
      "loss: 0.454678 | acc: 81.25%  [32000/48000]\n",
      "loss: 0.409916 | acc: 85.94%  [38400/48000]\n",
      "loss: 0.376674 | acc: 87.50%  [44800/48000]\n",
      "Val Avg loss: 0.394280 | val accuracy: 86.02%\n",
      "Test Avg loss: 0.420282 | Test accuracy: 84.83%\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.290177 | acc: 89.06%  [    0/48000]\n",
      "loss: 0.397736 | acc: 82.81%  [ 6400/48000]\n",
      "loss: 0.318013 | acc: 87.50%  [12800/48000]\n",
      "loss: 0.413474 | acc: 84.38%  [19200/48000]\n",
      "loss: 0.277789 | acc: 89.06%  [25600/48000]\n",
      "loss: 0.249193 | acc: 87.50%  [32000/48000]\n",
      "loss: 0.254345 | acc: 90.62%  [38400/48000]\n",
      "loss: 0.488280 | acc: 79.69%  [44800/48000]\n",
      "Val Avg loss: 0.411580 | val accuracy: 86.08%\n",
      "Test Avg loss: 0.439038 | Test accuracy: 85.27%\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.189497 | acc: 93.75%  [    0/48000]\n",
      "loss: 0.346262 | acc: 82.81%  [ 6400/48000]\n",
      "loss: 0.225258 | acc: 95.31%  [12800/48000]\n",
      "loss: 0.407210 | acc: 87.50%  [19200/48000]\n",
      "loss: 0.298253 | acc: 89.06%  [25600/48000]\n",
      "loss: 0.426473 | acc: 82.81%  [32000/48000]\n",
      "loss: 0.363369 | acc: 90.62%  [38400/48000]\n",
      "loss: 0.389788 | acc: 82.81%  [44800/48000]\n",
      "Val Avg loss: 0.348665 | val accuracy: 87.47%\n",
      "Test Avg loss: 0.376210 | Test accuracy: 86.45%\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 6.3 â€” run training\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(val_dataloader, train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Generated by ChatGPT"
   }
  ],
  "created": "2025-09-12T12:05:55.301308Z",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
