{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d89219",
   "metadata": {},
   "source": [
    "# üß™ PyTorch Lab 10: Solving Maze with Deep RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0541a",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "We use:\n",
    "- `gymnasium` for the environment wrapper\n",
    "- `torch` for the CNN policy/value network\n",
    "- `imageio` to convert recorded mp4 videos to gifs\n",
    "\n",
    "If you run locally, you may need ffmpeg installed for mp4 decoding; on Colab it usually works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792e00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip -q install gymnasium\n",
    "!pip -q install imageio imageio-ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ed6e3",
   "metadata": {},
   "source": [
    "### Device (CPU/GPU)\n",
    "We pick CUDA if available. This avoids runtime errors when GPU is not present.\n",
    "\n",
    "> If you **want to force GPU only** like in your original script, replace this with `device = \"cuda\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d912315",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d462b34",
   "metadata": {},
   "source": [
    "## 1. The environment: SimpleMaze10x10_4Actions + FrameStack4\n",
    "\n",
    "- The agent starts at `(1,1)` and must reach `(8,8)` (for size 10).\n",
    "- Reward structure:\n",
    "  - `-0.01` per step (time penalty)\n",
    "  - extra `-0.1` if bumping into a wall\n",
    "  - `+1.0` when reaching the goal\n",
    "- Observation: 84√ó84 grayscale image with:\n",
    "  - walls = dark gray\n",
    "  - goal = medium gray\n",
    "  - agent = white\n",
    "- We stack the last 4 frames ‚Üí observation becomes `(4,84,84)`.\n",
    "\n",
    "This mimics Atari-style inputs while staying tiny.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bf3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMaze10x10_4Actions(gym.Env):\n",
    "    '''\n",
    "    10x10 maze with Atari-like actions:\n",
    "      0=UP, 1=DOWN, 2=RIGHT, 3=LEFT\n",
    "\n",
    "    Observation:\n",
    "      single grayscale frame (84,84) uint8 (frame-stacked outside)\n",
    "    '''\n",
    "    metadata = {\"render_modes\": [\"rgb_array\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, size=10, max_steps=200, seed=0, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.max_steps = max_steps\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84), dtype=np.uint8)\n",
    "\n",
    "        self._build_fixed_maze()\n",
    "        self.reset(seed=seed)\n",
    "\n",
    "    def get_action_meanings(self):\n",
    "        return [\"UP\", \"DOWN\", \"RIGHT\", \"LEFT\"]\n",
    "\n",
    "    def _action_to_delta(self, a: int):\n",
    "        if a == 0:   # UP\n",
    "            return (-1, 0)\n",
    "        if a == 1:   # DOWN\n",
    "            return (1, 0)\n",
    "        if a == 2:   # RIGHT\n",
    "            return (0, 1)\n",
    "        if a == 3:   # LEFT\n",
    "            return (0, -1)\n",
    "        raise ValueError(a)\n",
    "\n",
    "    def _build_fixed_maze(self):\n",
    "        s = self.size\n",
    "        grid = np.zeros((s, s), dtype=np.uint8)\n",
    "\n",
    "        # border walls\n",
    "        grid[0, :] = 1\n",
    "        grid[-1, :] = 1\n",
    "        grid[:, 0] = 1\n",
    "        grid[:, -1] = 1\n",
    "\n",
    "        # internal walls (fixed layout)\n",
    "        walls = [\n",
    "            (2, 3), (2, 4),(2, 5),(2, 6),(2, 7),(2, 8),\n",
    "            (3, 3),\n",
    "            (5, 4), (5, 5), (5, 6),\n",
    "            (6, 4),(6, 6),\n",
    "            (7,4), (7, 5),(7, 6),\n",
    "        ]\n",
    "        for r, c in walls:\n",
    "            if 0 <= r < s and 0 <= c < s:\n",
    "                grid[r, c] = 1\n",
    "\n",
    "        self.grid = grid\n",
    "        self.start = (1, 1)\n",
    "        self.goal = (s - 2, s - 2)\n",
    "        self.grid[self.start] = 0\n",
    "        self.grid[self.goal] = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.pos = list(self.start)\n",
    "        self.steps = 0\n",
    "        obs = self._get_obs()\n",
    "        info = {\"pos\": tuple(self.pos)}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "\n",
    "        dr, dc = self._action_to_delta(int(action))\n",
    "        nr, nc = self.pos[0] + dr, self.pos[1] + dc\n",
    "\n",
    "        reward = -0.01\n",
    "        bumped = False\n",
    "\n",
    "        # wall collision\n",
    "        if self.grid[nr, nc] == 1:\n",
    "            bumped = True\n",
    "            reward -= 0.1\n",
    "        else:\n",
    "            self.pos = [nr, nc]\n",
    "\n",
    "        terminated = (tuple(self.pos) == self.goal)\n",
    "        if terminated:\n",
    "            reward = 1.0\n",
    "\n",
    "        truncated = (self.steps >= self.max_steps)\n",
    "        obs = self._get_obs()\n",
    "        info = {\"pos\": tuple(self.pos), \"bumped\": bumped}\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        H, W = 84, 84\n",
    "        img = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "        cell_h = H // self.size\n",
    "        cell_w = W // self.size\n",
    "\n",
    "        # draw walls\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                if self.grid[r, c] == 1:\n",
    "                    y0, y1 = r * cell_h, (r + 1) * cell_h\n",
    "                    x0, x1 = c * cell_w, (c + 1) * cell_w\n",
    "                    img[y0:y1, x0:x1] = 60\n",
    "\n",
    "        # goal\n",
    "        gr, gc = self.goal\n",
    "        y0, y1 = gr * cell_h, (gr + 1) * cell_h\n",
    "        x0, x1 = gc * cell_w, (gc + 1) * cell_w\n",
    "        img[y0:y1, x0:x1] = 160\n",
    "\n",
    "        # agent\n",
    "        ar, ac = self.pos\n",
    "        y0, y1 = ar * cell_h, (ar + 1) * cell_h\n",
    "        x0, x1 = ac * cell_w, (ac + 1) * cell_w\n",
    "        img[y0:y1, x0:x1] = 255\n",
    "\n",
    "        return img\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            g = self._get_obs()\n",
    "            return np.stack([g, g, g], axis=-1)\n",
    "        return None\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FrameStack4(gym.Wrapper):\n",
    "    def __init__(self, env, k=4):\n",
    "        super().__init__(env)\n",
    "        self.k = k\n",
    "        self.frames = None\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(k, 84, 84), dtype=np.uint8)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.frames = [obs.copy() for _ in range(self.k)]\n",
    "        return self._get(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.pop(0)\n",
    "        self.frames.append(obs.copy())\n",
    "        return self._get(), reward, terminated, truncated, info\n",
    "\n",
    "    def _get(self):\n",
    "        return np.stack(self.frames, axis=0)  # (4,84,84)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe07f1",
   "metadata": {},
   "source": [
    "### Helpers: make_env + video helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(record=False, seed=0, max_steps=200):\n",
    "    render_mode = \"rgb_array\" if record else None\n",
    "    env = SimpleMaze10x10_4Actions(size=10, max_steps=max_steps, seed=seed, render_mode=render_mode)\n",
    "    env = FrameStack4(env, k=4)\n",
    "\n",
    "    if record:\n",
    "        env = gym.wrappers.RecordVideo(env, video_folder=\"videos\", episode_trigger=lambda i: True)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def newest_mp4(folder):\n",
    "    mp4s = sorted(glob.glob(os.path.join(folder, \"*.mp4\")), key=os.path.getmtime)\n",
    "    if not mp4s:\n",
    "        raise FileNotFoundError(f\"No mp4 found in {folder}\")\n",
    "    return mp4s[-1]\n",
    "\n",
    "\n",
    "def mp4_to_gif(mp4_path, gif_path, fps=30):\n",
    "    reader = imageio.get_reader(mp4_path)\n",
    "    frames = [frame for frame in reader]\n",
    "    reader.close()\n",
    "    imageio.mimsave(gif_path, frames, fps=fps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031dfda",
   "metadata": {},
   "source": [
    "## 2. Visual sanity check\n",
    "Reset the environment and display the most recent frame in the stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4af06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = make_env(record=False, seed=0)\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(obs[-1], cmap=\"gray\", vmin=0, vmax=255)\n",
    "plt.title(f\"Start pos: {info['pos']}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db87286",
   "metadata": {},
   "source": [
    "## 3. The network (CNN84)\n",
    "Atari-style CNN trunk + two heads:\n",
    "- policy logits over 4 actions\n",
    "- value estimate V(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN84(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.p = nn.Linear(512, 4)\n",
    "        self.v = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        logits = self.p(x)\n",
    "        value = self.v(x).squeeze(1)\n",
    "        return logits, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773eae5",
   "metadata": {},
   "source": [
    "## 4. Monte‚ÄëCarlo returns\n",
    "Compute discounted returns backwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def discounted_returns(rewards: torch.Tensor, gamma: float) -> torch.Tensor:\n",
    "    T = rewards.shape[0]\n",
    "    returns = torch.empty_like(rewards)\n",
    "    G = torch.zeros((), device=rewards.device, dtype=rewards.dtype)\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        G = rewards[t] + gamma * G\n",
    "        returns[t] = G\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e8f9f",
   "metadata": {},
   "source": [
    "## 5. Collecting one episode\n",
    "Roll out using the current stochastic policy and store (obs, action, reward).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcceab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, model):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    ep_return = 0.0\n",
    "    episode_buffer = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            x = (torch.tensor(obs).unsqueeze(0).float() / 255.0).to(device)\n",
    "            logits, _ = model(x)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            episode_buffer.append((obs, action, reward))\n",
    "            ep_return += reward\n",
    "\n",
    "    return episode_buffer, ep_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f049a77",
   "metadata": {},
   "source": [
    "## 6. Part A ‚Äî Simple REINFORCE (baseline)\n",
    "\n",
    "REINFORCE gradient:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\, G_t\n",
    "$$\n",
    "\n",
    "We minimize:\n",
    "$$\n",
    "\\mathcal{L}_\\text{actor} = -\\mathbb{E}[\\log \\pi(a_t|s_t)\\, G_t]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_batch_reinforce(model, episodes, optimizer, gamma=0.99, normalize_returns=True):\n",
    "    obs_list, act_list, G_list = [], [], []\n",
    "\n",
    "    for episode_buffer in episodes:\n",
    "        obs, action, reward = map(np.array, zip(*episode_buffer))\n",
    "        G = discounted_returns(torch.tensor(reward, dtype=torch.float32, device=device), gamma=gamma)\n",
    "\n",
    "        obs_list.append(obs)\n",
    "        act_list.append(action)\n",
    "        G_list.append(G)\n",
    "\n",
    "    obs = np.concatenate(obs_list, axis=0)\n",
    "    action = np.concatenate(act_list, axis=0)\n",
    "    G = torch.cat(G_list, dim=0)\n",
    "\n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device) / 255.0\n",
    "    action = torch.tensor(action, dtype=torch.long, device=device)\n",
    "\n",
    "    logits, _ = model(obs)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    logp = dist.log_prob(action)\n",
    "\n",
    "    if normalize_returns:\n",
    "        G = (G - G.mean()) / (G.std() + 1e-8)\n",
    "\n",
    "    loss_actor = -(logp * G).mean()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss_actor.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss_actor.item(), dist.entropy().mean().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bac33c",
   "metadata": {},
   "source": [
    "### Train REINFORCE\n",
    "We update every `BATCH_EPISODES` episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b72111",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR = \"videos\"\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "N_EPISODES = 300\n",
    "BATCH_EPISODES = 5\n",
    "GAMMA = 0.99\n",
    "LR = 2e-4\n",
    "\n",
    "model = CNN84().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "env = make_env(record=True, seed=0)\n",
    "\n",
    "rewards, losses, entropies = [], [], []\n",
    "batch_buffer = []\n",
    "\n",
    "for i in tqdm(range(N_EPISODES)):\n",
    "    episode_buffer, r = run_episode(env, model)\n",
    "\n",
    "    # ---- CORRECTION kept as comment (remove after you verify) ----\n",
    "    # batch_buffer.extend(episode_buffer)  # ‚ùå wrong type: would flatten transitions; update expects list of episodes\n",
    "    batch_buffer.append(episode_buffer)    # ‚úÖ each element is one episode (list of transitions)\n",
    "\n",
    "    rewards.append(r)\n",
    "\n",
    "    if (i + 1) % BATCH_EPISODES == 0:\n",
    "        l, e = update_batch_reinforce(model, batch_buffer, optimizer, gamma=GAMMA)\n",
    "        batch_buffer = []\n",
    "        losses.append(l)\n",
    "        entropies.append(e)\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(\n",
    "            f\"Episode {i+1} | \"\n",
    "            f\"Avg Reward (50): {np.mean(rewards[-50:]):.2f} | \"\n",
    "            f\"Last Entropy: {entropies[-1] if entropies else float('nan'):.2f} | \"\n",
    "            f\"Last Loss: {losses[-1] if losses else float('nan'):.4f}\"\n",
    "        )\n",
    "        try:\n",
    "            mp4_path = newest_mp4(VIDEO_DIR)\n",
    "            gif_path = f\"reinforce_ep{i+1}.gif\"\n",
    "            mp4_to_gif(mp4_path, gif_path, fps=30)\n",
    "            print(\"Saved gif:\", gif_path)\n",
    "        except Exception as ex:\n",
    "            print(\"GIF export skipped:\", ex)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74a1b7",
   "metadata": {},
   "source": [
    "### Plot learning curves (REINFORCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Episode return (REINFORCE)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Actor loss per update (REINFORCE)\")\n",
    "plt.xlabel(\"Update step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636eaf1",
   "metadata": {},
   "source": [
    "## 7. Part B ‚Äî Variance reduction: baseline with a learned V(s)\n",
    "\n",
    "Your method:\n",
    "- Advantage: `A = G - V(s)` (with `V` detached in actor term)\n",
    "- Normalize `A` in the batch\n",
    "- Add a critic MSE loss to fit `V(s) ‚âà G`\n",
    "- Total: `loss = loss_actor + 0.5 * loss_critic`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff20d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_batch_actor_critic_baseline(model, episodes, optimizer, gamma=0.99):\n",
    "    obs_list, act_list, G_list = [], [], []\n",
    "\n",
    "    for episode_buffer in episodes:\n",
    "        obs, action, reward = map(np.array, zip(*episode_buffer))\n",
    "        G = discounted_returns(torch.tensor(reward, dtype=torch.float32, device=device), gamma=gamma)\n",
    "\n",
    "        obs_list.append(obs)\n",
    "        act_list.append(action)\n",
    "        G_list.append(G)\n",
    "\n",
    "    obs = np.concatenate(obs_list, axis=0)\n",
    "    action = np.concatenate(act_list, axis=0)\n",
    "    G = torch.cat(G_list, dim=0)\n",
    "\n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device) / 255.0\n",
    "    action = torch.tensor(action, dtype=torch.long, device=device)\n",
    "\n",
    "    logits, V = model(obs)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    logp = dist.log_prob(action)\n",
    "\n",
    "    A = G - V.detach()\n",
    "    A = (A - A.mean()) / (A.std() + 1e-8)\n",
    "\n",
    "    loss_actor = -(logp * A).mean()\n",
    "    loss_critic = F.mse_loss(V, G)\n",
    "    loss = loss_actor + 0.5 * loss_critic\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), loss_actor.item(), loss_critic.item(), dist.entropy().mean().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13e8ec",
   "metadata": {},
   "source": [
    "### Train with baseline (re-init model for comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = CNN84().to(device)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=LR)\n",
    "\n",
    "N_EPISODES_2 = 300\n",
    "BATCH_EPISODES_2 = 5\n",
    "\n",
    "env = make_env(record=True, seed=0)\n",
    "\n",
    "rewards2, losses2, actor_losses2, critic_losses2, entropies2 = [], [], [], [], []\n",
    "batch_buffer = []\n",
    "\n",
    "for i in tqdm(range(N_EPISODES_2)):\n",
    "    episode_buffer, r = run_episode(env, model2)\n",
    "\n",
    "    # ---- CORRECTION kept as comment (remove after you verify) ----\n",
    "    # batch_buffer.extend(episode_buffer)  # ‚ùå wrong type: would flatten transitions; update expects list of episodes\n",
    "    batch_buffer.append(episode_buffer)    # ‚úÖ each element is one episode (list of transitions)\n",
    "\n",
    "    rewards2.append(r)\n",
    "\n",
    "    if (i + 1) % BATCH_EPISODES_2 == 0:\n",
    "        l, la, lc, e = update_batch_actor_critic_baseline(model2, batch_buffer, optimizer2, gamma=GAMMA)\n",
    "        batch_buffer = []\n",
    "        losses2.append(l)\n",
    "        actor_losses2.append(la)\n",
    "        critic_losses2.append(lc)\n",
    "        entropies2.append(e)\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(\n",
    "            f\"Episode {i+1} | \"\n",
    "            f\"Avg Reward (50): {np.mean(rewards2[-50:]):.2f} | \"\n",
    "            f\"Entropy: {entropies2[-1] if entropies2 else float('nan'):.2f} | \"\n",
    "            f\"Loss: {losses2[-1] if losses2 else float('nan'):.4f} | \"\n",
    "            f\"Actor: {actor_losses2[-1] if actor_losses2 else float('nan'):.4f} | \"\n",
    "            f\"Critic: {critic_losses2[-1] if critic_losses2 else float('nan'):.4f}\"\n",
    "        )\n",
    "        try:\n",
    "            mp4_path = newest_mp4(VIDEO_DIR)\n",
    "            gif_path = f\"baseline_ep{i+1}.gif\"\n",
    "            mp4_to_gif(mp4_path, gif_path, fps=30)\n",
    "            print(\"Saved gif:\", gif_path)\n",
    "        except Exception as ex:\n",
    "            print(\"GIF export skipped:\", ex)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f692b83",
   "metadata": {},
   "source": [
    "### Plot learning curves (baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd50b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(rewards2)\n",
    "plt.title(\"Episode return (Baseline / Actor-Critic style)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses2, label=\"total\")\n",
    "plt.plot(actor_losses2, label=\"actor\")\n",
    "plt.plot(critic_losses2, label=\"critic\")\n",
    "plt.title(\"Losses per update (Baseline / Actor-Critic style)\")\n",
    "plt.xlabel(\"Update step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf413dc",
   "metadata": {},
   "source": [
    "## 8. Compare REINFORCE vs Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d03ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(rewards, label=\"REINFORCE\")\n",
    "plt.plot(rewards2, label=\"Baseline (A=G-V)\")\n",
    "plt.title(\"Episode return comparison\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9675ca",
   "metadata": {},
   "source": [
    "## 9. Record a final episode (GIF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_env = make_env(record=True, seed=42)\n",
    "episode_buffer, r = run_episode(record_env, model2)\n",
    "record_env.close()\n",
    "print(f\"Recorded episode return: {r}\")\n",
    "\n",
    "mp4_path = newest_mp4(VIDEO_DIR)\n",
    "mp4_to_gif(mp4_path, \"last_episode.gif\", fps=30)\n",
    "print(\"Saved gif: last_episode.gif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482578df",
   "metadata": {},
   "source": [
    "## Exercises (optional)\n",
    "1. Reward shaping: change wall penalty or step penalty.\n",
    "2. Add an entropy bonus term.\n",
    "3. Replace Monte‚ÄëCarlo returns with TD(0) targets.\n",
    "4. Change frame stack size (k=1 vs k=4).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
