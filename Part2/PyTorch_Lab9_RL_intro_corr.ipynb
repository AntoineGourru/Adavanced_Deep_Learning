{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7017216b",
   "metadata": {},
   "source": [
    "# ðŸ§ª PyTorch Lab 9: Intro to Deep RL\n",
    "\n",
    "This notebook is a **very simple introduction** to Reinforcement Learning (RL) concepts **without learning** a policy.\n",
    "\n",
    "Goal: understand the interaction loop:\n",
    "\n",
    "\\[\n",
    "(s_t) \\xrightarrow{a_t} (r_t, s_{t+1}, \\text{done})\n",
    "\\]\n",
    "\n",
    "We will:\n",
    "1. Build a tiny Maze environment (same as your later notebook).\n",
    "2. Step through episodes with:\n",
    "   - a **random policy**\n",
    "   - a **hand-coded heuristic policy**\n",
    "3. Inspect trajectories, returns, and what â€œepisode terminationâ€ means.\n",
    "4. Record a short rollout as a **GIF**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c120f7",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "We use `gymnasium` to wrap an environment, plus `matplotlib` for visualization and `imageio` for GIF export.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264cfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install gymnasium\n",
    "!pip -q install imageio imageio-ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d041ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cda037",
   "metadata": {},
   "source": [
    "## 1. Environment: a small maze with 4 actions\n",
    "\n",
    "Actions:\n",
    "- `0 = UP`\n",
    "- `1 = DOWN`\n",
    "- `2 = RIGHT`\n",
    "- `3 = LEFT`\n",
    "\n",
    "Observation:\n",
    "- a grayscale 84Ã—84 image (like Atari), so we can visualize easily.\n",
    "\n",
    "Reward:\n",
    "- `-0.01` every step (encourages shorter paths)\n",
    "- `-0.1` if you bump into a wall\n",
    "- `+1.0` if you reach the goal\n",
    "\n",
    "Episode ends if:\n",
    "- you reach the goal (`terminated=True`), or\n",
    "- you hit the step limit (`truncated=True`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0821963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMaze10x10_4Actions(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"rgb_array\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, size=10, max_steps=200, seed=0, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.max_steps = max_steps\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84), dtype=np.uint8)\n",
    "\n",
    "        self._build_fixed_maze()\n",
    "        self.reset(seed=seed)\n",
    "\n",
    "    def get_action_meanings(self):\n",
    "        return [\"UP\", \"DOWN\", \"RIGHT\", \"LEFT\"]\n",
    "\n",
    "    def _action_to_delta(self, a: int):\n",
    "        if a == 0: return (-1, 0)  # UP\n",
    "        if a == 1: return ( 1, 0)  # DOWN\n",
    "        if a == 2: return ( 0, 1)  # RIGHT\n",
    "        if a == 3: return ( 0,-1)  # LEFT\n",
    "        raise ValueError(a)\n",
    "\n",
    "    def _build_fixed_maze(self):\n",
    "        s = self.size\n",
    "        grid = np.zeros((s, s), dtype=np.uint8)\n",
    "\n",
    "        # border walls\n",
    "        grid[0, :] = 1\n",
    "        grid[-1, :] = 1\n",
    "        grid[:, 0] = 1\n",
    "        grid[:, -1] = 1\n",
    "\n",
    "        # internal walls (fixed layout)\n",
    "        walls = [\n",
    "            (2, 3), (2, 4),(2, 5),(2, 6),(2, 7),(2, 8),\n",
    "            (3, 3),\n",
    "            (5, 4), (5, 5), (5, 6),\n",
    "            (6, 4),(6, 6),\n",
    "            (7,4), (7, 5),(7, 6),\n",
    "        ]\n",
    "        for r, c in walls:\n",
    "            grid[r, c] = 1\n",
    "\n",
    "        self.grid = grid\n",
    "        self.start = (1, 1)\n",
    "        self.goal = (s - 2, s - 2)\n",
    "\n",
    "        self.grid[self.start] = 0\n",
    "        self.grid[self.goal] = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.pos = list(self.start)\n",
    "        self.steps = 0\n",
    "        obs = self._get_obs()\n",
    "        info = {\"pos\": tuple(self.pos)}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "\n",
    "        dr, dc = self._action_to_delta(int(action))\n",
    "        nr, nc = self.pos[0] + dr, self.pos[1] + dc\n",
    "\n",
    "        reward = -0.01\n",
    "        bumped = False\n",
    "\n",
    "        if self.grid[nr, nc] == 1:\n",
    "            bumped = True\n",
    "            reward -= 0.1\n",
    "        else:\n",
    "            self.pos = [nr, nc]\n",
    "\n",
    "        terminated = (tuple(self.pos) == self.goal)\n",
    "        if terminated:\n",
    "            reward = 1.0\n",
    "\n",
    "        truncated = (self.steps >= self.max_steps)\n",
    "        obs = self._get_obs()\n",
    "        info = {\"pos\": tuple(self.pos), \"bumped\": bumped}\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        H, W = 84, 84\n",
    "        img = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "        cell_h = H // self.size\n",
    "        cell_w = W // self.size\n",
    "\n",
    "        # walls\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                if self.grid[r, c] == 1:\n",
    "                    y0, y1 = r * cell_h, (r + 1) * cell_h\n",
    "                    x0, x1 = c * cell_w, (c + 1) * cell_w\n",
    "                    img[y0:y1, x0:x1] = 60\n",
    "\n",
    "        # goal\n",
    "        gr, gc = self.goal\n",
    "        y0, y1 = gr * cell_h, (gr + 1) * cell_h\n",
    "        x0, x1 = gc * cell_w, (gc + 1) * cell_w\n",
    "        img[y0:y1, x0:x1] = 160\n",
    "\n",
    "        # agent\n",
    "        ar, ac = self.pos\n",
    "        y0, y1 = ar * cell_h, (ar + 1) * cell_h\n",
    "        x0, x1 = ac * cell_w, (ac + 1) * cell_w\n",
    "        img[y0:y1, x0:x1] = 255\n",
    "\n",
    "        return img\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            g = self._get_obs()\n",
    "            return np.stack([g, g, g], axis=-1)\n",
    "        return None\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852961e",
   "metadata": {},
   "source": [
    "## 2. First contact: reset, inspect the observation, inspect actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleMaze10x10_4Actions(seed=0)\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(\"Start position:\", info[\"pos\"])\n",
    "print(\"Action meanings:\", env.get_action_meanings())\n",
    "print(\"Observation shape:\", obs.shape, obs.dtype)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(obs, cmap=\"gray\", vmin=0, vmax=255)\n",
    "plt.title(\"Initial observation (start)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2af4a",
   "metadata": {},
   "source": [
    "## 3. The RL interaction loop (one episode, random policy)\n",
    "\n",
    "A **policy** is just a rule that chooses actions.\n",
    "Here: pick a random action at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a123c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_random(env, max_steps=50, seed=None):\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    traj = []\n",
    "    total_return = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        traj.append({\n",
    "            \"t\": t,\n",
    "            \"action\": action,\n",
    "            \"action_name\": env.get_action_meanings()[action],\n",
    "            \"reward\": reward,\n",
    "            \"pos\": info[\"pos\"],\n",
    "            \"bumped\": info.get(\"bumped\", False),\n",
    "            \"done\": done,\n",
    "            \"terminated\": terminated,\n",
    "            \"truncated\": truncated,\n",
    "        })\n",
    "        total_return += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return traj, total_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2809f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleMaze10x10_4Actions(seed=0, max_steps=200)\n",
    "traj, G = run_episode_random(env, max_steps=80, seed=0)\n",
    "\n",
    "print(\"Episode length:\", len(traj))\n",
    "print(\"Return (sum of rewards):\", round(G, 3))\n",
    "print(\"Last transition:\", traj[-1])\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8422c856",
   "metadata": {},
   "source": [
    "### Visualize a few steps\n",
    "We show a few frames to see movement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7440751",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleMaze10x10_4Actions(seed=0, max_steps=200)\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "frames = [obs]\n",
    "for step in traj:\n",
    "    obs, reward, terminated, truncated, info = env.step(step[\"action\"])\n",
    "    frames.append(obs)\n",
    "    if step[\"done\"]:\n",
    "        break\n",
    "\n",
    "show_steps = [0, 1, 2, 5, 10, 20]\n",
    "plt.figure(figsize=(12,6))\n",
    "for i, t in enumerate(show_steps):\n",
    "    if t < len(frames):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(frames[t], cmap=\"gray\", vmin=0, vmax=255)\n",
    "        plt.title(f\"t={t}\")\n",
    "        plt.axis(\"off\")\n",
    "plt.suptitle(\"Random rollout snapshots\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd4972",
   "metadata": {},
   "source": [
    "## 4. A hand-coded heuristic policy (not learning)\n",
    "\n",
    "We write a simple â€œgoal-seekingâ€ policy:\n",
    "- try to move RIGHT / DOWN toward the goal\n",
    "- avoid walls by choosing the first valid move among candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a934311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_towards_goal_candidates(env):\n",
    "    r, c = env.pos\n",
    "    gr, gc = env.goal\n",
    "\n",
    "    candidates = []\n",
    "    if gc > c: candidates.append(2)  # RIGHT\n",
    "    if gr > r: candidates.append(1)  # DOWN\n",
    "    if gc < c: candidates.append(3)  # LEFT\n",
    "    if gr < r: candidates.append(0)  # UP\n",
    "\n",
    "    for a in [2, 1, 3, 0]:  # fallback order\n",
    "        if a not in candidates:\n",
    "            candidates.append(a)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def run_episode_heuristic(env, max_steps=200, seed=None):\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    traj = []\n",
    "    total_return = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        chosen = None\n",
    "        for a in greedy_towards_goal_candidates(env):\n",
    "            dr, dc = env._action_to_delta(a)\n",
    "            nr, nc = env.pos[0] + dr, env.pos[1] + dc\n",
    "            if env.grid[nr, nc] == 0:\n",
    "                chosen = a\n",
    "                break\n",
    "        if chosen is None:\n",
    "            chosen = env.action_space.sample()\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(chosen)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        traj.append({\n",
    "            \"t\": t,\n",
    "            \"action\": chosen,\n",
    "            \"action_name\": env.get_action_meanings()[chosen],\n",
    "            \"reward\": reward,\n",
    "            \"pos\": info[\"pos\"],\n",
    "            \"bumped\": info.get(\"bumped\", False),\n",
    "            \"done\": done,\n",
    "        })\n",
    "        total_return += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return traj, total_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e29da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleMaze10x10_4Actions(seed=0, max_steps=200)\n",
    "traj_h, G_h = run_episode_heuristic(env, seed=0)\n",
    "\n",
    "print(\"Heuristic episode length:\", len(traj_h))\n",
    "print(\"Heuristic return:\", round(G_h, 3))\n",
    "print(\"Reached goal?\", traj_h[-1][\"pos\"] == env.goal)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfbc37",
   "metadata": {},
   "source": [
    "## 5. Compare policies statistically (no learning)\n",
    "\n",
    "We run many episodes and compare:\n",
    "- mean return\n",
    "- mean episode length\n",
    "- success rate (reach goal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy_runner, n_episodes=50, seed0=0):\n",
    "    returns, lengths = [], []\n",
    "    successes = 0\n",
    "\n",
    "    env = SimpleMaze10x10_4Actions(seed=seed0, max_steps=200)\n",
    "    for i in range(n_episodes):\n",
    "        traj, G = policy_runner(env, seed=seed0+i)\n",
    "        returns.append(G)\n",
    "        lengths.append(len(traj))\n",
    "        if traj and traj[-1][\"pos\"] == env.goal:\n",
    "            successes += 1\n",
    "    env.close()\n",
    "\n",
    "    return {\n",
    "        \"mean_return\": float(np.mean(returns)),\n",
    "        \"std_return\": float(np.std(returns)),\n",
    "        \"mean_length\": float(np.mean(lengths)),\n",
    "        \"success_rate\": successes / n_episodes,\n",
    "    }\n",
    "\n",
    "stats_random = evaluate(lambda env, seed: run_episode_random(env, max_steps=200, seed=seed), n_episodes=50, seed0=0)\n",
    "stats_heur  = evaluate(lambda env, seed: run_episode_heuristic(env, max_steps=200, seed=seed), n_episodes=50, seed0=0)\n",
    "\n",
    "stats_random, stats_heur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed19bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Random\", \"Heuristic\"]\n",
    "mean_returns = [stats_random[\"mean_return\"], stats_heur[\"mean_return\"]]\n",
    "mean_lengths = [stats_random[\"mean_length\"], stats_heur[\"mean_length\"]]\n",
    "success = [stats_random[\"success_rate\"], stats_heur[\"success_rate\"]]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, mean_returns)\n",
    "plt.title(\"Mean return (higher is better)\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, mean_lengths)\n",
    "plt.title(\"Mean episode length (lower is better)\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, success)\n",
    "plt.title(\"Success rate (reach goal)\")\n",
    "plt.ylabel(\"Rate\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa132ff9",
   "metadata": {},
   "source": [
    "## 6. Record one rollout as a GIF\n",
    "\n",
    "We record an episode using `RecordVideo` (mp4) then convert to gif.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newest_mp4(folder):\n",
    "    mp4s = sorted(glob.glob(os.path.join(folder, \"*.mp4\")), key=os.path.getmtime)\n",
    "    if not mp4s:\n",
    "        raise FileNotFoundError(f\"No mp4 found in {folder}\")\n",
    "    return mp4s[-1]\n",
    "\n",
    "def mp4_to_gif(mp4_path, gif_path, fps=30):\n",
    "    reader = imageio.get_reader(mp4_path)\n",
    "    frames = [frame for frame in reader]\n",
    "    reader.close()\n",
    "    imageio.mimsave(gif_path, frames, fps=fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR = \"videos_intro\"\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "env = SimpleMaze10x10_4Actions(seed=0, max_steps=200, render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=VIDEO_DIR, episode_trigger=lambda i: True)\n",
    "\n",
    "traj_gif, G_gif = run_episode_heuristic(env, seed=0)\n",
    "env.close()\n",
    "\n",
    "mp4_path = newest_mp4(VIDEO_DIR)\n",
    "gif_path = \"intro_heuristic.gif\"\n",
    "mp4_to_gif(mp4_path, gif_path, fps=30)\n",
    "\n",
    "print(\"Heuristic return:\", round(G_gif, 3))\n",
    "print(\"Saved gif:\", gif_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be958f",
   "metadata": {},
   "source": [
    "## Exercises (optional)\n",
    "\n",
    "1. Modify the maze: add/remove a wall and see if the heuristic still works.\n",
    "2. Change the reward shaping: increase wall penalty and see how random returns change.\n",
    "3. Design a better hand-coded policy (wall-following, memory of visited cells).\n",
    "4. Add randomness: with probability 0.1, take a random action even in the heuristic policy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
